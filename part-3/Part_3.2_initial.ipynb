{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f5d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''skip-gram方法训练词向量, 给定中心词预测上下文'''\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud\n",
    "from torch.nn.parameter import Parameter\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from paddlenlp import Taskflow\n",
    "import json\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # 余弦相似度函数\n",
    "\n",
    "\n",
    "# Dataloader\n",
    "class Dataset(tud.Dataset):  # 继承tud.Dataset父类\n",
    "\n",
    "    def __init__(self, text, word_to_idx, idx_to_word, word_freqs, word_counts):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.text_encoded = [word_to_idx.get(t, VOCAB_SIZE - 1) for t in text]\n",
    "        self.text_encoded = torch.Tensor(self.text_encoded).long()\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = idx_to_word\n",
    "        self.word_freqs = torch.Tensor(word_freqs)\n",
    "        self.word_counts = torch.Tensor(word_counts)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_encoded)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ''' 这个function返回以下数据用于训练\n",
    "            - 中心词\n",
    "            - 这个单词附近的(positive)单词\n",
    "            - 随机采样的K个单词作为negative sample\n",
    "        '''\n",
    "        center_word = self.text_encoded[idx]\n",
    "        pos_indices = list(range(idx - C, idx)) + list(range(idx + 1, idx + C + 1))\n",
    "        pos_indices = [i % len(self.text_encoded) for i in pos_indices]\n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "        # replacement=True有放回的取\n",
    "        neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0], replacement=True)\n",
    "\n",
    "        return center_word, pos_words, neg_words\n",
    "\n",
    "    # skip-gram model\n",
    "\n",
    "\n",
    "class EmbeddingModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.vocab_size = vocab_size  # 30000\n",
    "        self.embed_size = embed_size  # 100\n",
    "        # 模型输入，输出是两个一样的矩阵参数nn.Embedding(30000, 100)\n",
    "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
    "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
    "        # 模型权重初始化\n",
    "        initrange = 0.5 / self.embed_size\n",
    "        self.in_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out_embed.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        '''\n",
    "        input_labels: 中心词,         [batch_size]\n",
    "        pos_labels: 中心词周围词       [batch_size * (c * 2)]\n",
    "        neg_labelss: 中心词负采样单词  [batch_size, (c * 2 * K)]\n",
    "        return: loss, 返回loss        [batch_size]\n",
    "        '''\n",
    "        batch_size = input_labels.size(0)\n",
    "        input_embedding = self.in_embed(input_labels)  # B * embed_size\n",
    "        pos_embedding = self.out_embed(pos_labels)  # B * (2C) * embed_size\n",
    "        neg_embedding = self.out_embed(neg_labels)  # B * (2*C*K) * embed_size\n",
    "\n",
    "        # torch.bmm()为batch间的矩阵相乘（b,n.m)*(b,m,p)=(b,n,p)\n",
    "        log_pos = torch.bmm(pos_embedding, input_embedding.unsqueeze(2)).squeeze()  # B * (2*C)\n",
    "        log_neg = torch.bmm(neg_embedding, -input_embedding.unsqueeze(2)).squeeze()  # B * (2*C*K)\n",
    "\n",
    "        # 下面loss计算就是论文里的公式\n",
    "        log_pos = F.logsigmoid(log_pos).sum(1)  # batch_size\n",
    "        log_neg = F.logsigmoid(log_neg).sum(1)  # batch_size\n",
    "        loss = log_pos + log_neg  # 正样本损失和负样本损失和尽量最大\n",
    "        return -loss\n",
    "\n",
    "        # 模型训练有两个矩阵self.in_embed和self.out_embed, 作者认为输入矩阵比较好\n",
    "\n",
    "    def input_embeddings(self):\n",
    "        return self.in_embed.weight.data.cpu().numpy()\n",
    "\n",
    "\n",
    "def evaluate(filename, embedding_weights):\n",
    "    # embedding_weights是训练之后的embedding向量\n",
    "    model_similarity = []\n",
    "\n",
    "def openreadtxt(file_name):\n",
    "    data = []\n",
    "    file = open(file_name, 'r', encoding='UTF-8')\n",
    "    file_data = file.readlines()\n",
    "    for row in file_data:\n",
    "        data.append(row)\n",
    "    return data\n",
    "\n",
    "    for i in data.iloc[:, 0:2].index:\n",
    "        word1, word2 = data.iloc[i, 0], data.iloc[i, 1]\n",
    "        if word1 not in word_to_idx or word2 not in word_to_idx:\n",
    "            continue\n",
    "        else:\n",
    "            word1_idx, word2_idx = word_to_idx[word1], word_to_idx[word2]\n",
    "            word1_embed, word2_embed = embedding_weights[[word1_idx]], embedding_weights[[word2_idx]]\n",
    "            # 在分别取出这两个单词对应的embedding向量，具体为啥是这种取出方式[[word1_idx]]，可以自行研究\n",
    "            model_similarity.append(float(sklearn.metrics.pairwise.cosine_similarity(word1_embed, word2_embed)))\n",
    "            # 用余弦相似度计算这两个100维向量的相似度。这个是模型算出来的相似度\n",
    "\n",
    "\n",
    "    return scipy.stats.spearmanr(model_similarity)  # model_similarity\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    random.seed(1234)\n",
    "    np.random.seed(1234)\n",
    "    torch.manual_seed(1234)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(1234)\n",
    "\n",
    "    # 超参数设置\n",
    "    K = 10  # 负样本随机采样数量\n",
    "    C = 3  # 周围单词的数量\n",
    "    NUM_EPOCHS = 2\n",
    "    VOCAB_SIZE = 30000\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.2\n",
    "    EMBEDDING_SIZE = 100\n",
    "    LOG_FILE = \"word_embedding.log\"\n",
    "\n",
    "    with open(\"text_0.txt\", \"r\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    text = [w for w in (text.lower()).split()]\n",
    "    vocab = dict(Counter(text).most_common(VOCAB_SIZE - 1))\n",
    "    vocab[\"<unk>\"] = len(text) - np.sum(list(vocab.values()))\n",
    "    idx_to_word = [word for word in vocab.keys()]\n",
    "    word_to_idx = {word: i for i, word in enumerate(idx_to_word)}\n",
    "\n",
    "    word_counts = np.array([count for count in vocab.values()], dtype=np.float32)\n",
    "    word_freqs = word_counts / np.sum(word_counts)\n",
    "    word_freqs = word_freqs ** (3. / 4.)\n",
    "    word_freqs = word_freqs / np.sum(word_freqs)  # 用来做 negative sampling\n",
    "\n",
    "    dataset = Dataset(text, word_to_idx, idx_to_word, word_freqs, word_counts)\n",
    "    dataloader = tud.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    # print(next(iter(dataloader))[0].shape) # 中间词维度data\n",
    "    # print(next(iter(dataloader))[1].shape) # 周围词维度\n",
    "    # print(next(iter(dataloader))[2].shape) # 负样本维度\n",
    "\n",
    "    model = EmbeddingModel(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    # training\n",
    "    for e in range(NUM_EPOCHS):\n",
    "        for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "            input_labels = input_labels.long()\n",
    "            pos_labels = pos_labels.long()\n",
    "            neg_labels = neg_labels.long()\n",
    "            if use_cuda:\n",
    "                input_labels = input_labels.cuda()\n",
    "                pos_labels = pos_labels.cuda()\n",
    "                neg_labels = neg_labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(input_labels, pos_labels, neg_labels).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                with open(LOG_FILE, \"a\") as fout:\n",
    "                    fout.write(\"epoch: {}, iter: {}, loss: {}\\n\".format(e, i, loss.item()))\n",
    "                    print(\"epoch: {}, iter: {}, loss: {}\".format(e, i, loss.item()))\n",
    "\n",
    "            if i % 2000 == 0:\n",
    "                embedding_weights = model.input_embeddings()  # 取出训练中的in_embed词向量\n",
    "                # 在三个词文本上评估词向量\n",
    "                sim_simlex = evaluate(\"simlex-999.txt\", embedding_weights)\n",
    "                sim_men = evaluate(\"men.txt\", embedding_weights)\n",
    "                sim_353 = evaluate(\"wordsim353.csv\", embedding_weights)\n",
    "\n",
    "                with open(LOG_FILE, \"a\") as fout:\n",
    "                    print(\"epoch: {}, iter: {}, simlex-999: {}, men: {}, sim353: {}, nearest to monster: {}\\n\".format(\n",
    "                        e, i, sim_simlex, sim_men, sim_353, find_nearest(\"monster\")))\n",
    "                    fout.write(\n",
    "                        \"epoch: {}, iter: {}, simlex-999: {}, men: {}, sim353: {}, nearest to monster: {}\\n\".format(\n",
    "                            e, i, sim_simlex, sim_men, sim_353, find_nearest(\"monster\")))\n",
    "\n",
    "        embedding_weights = model.input_embeddings()  # 调用最终训练好的embeding词向量\n",
    "        torch.save(model.state_dict(), 'model_embedding.th')  # 模型保存"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
