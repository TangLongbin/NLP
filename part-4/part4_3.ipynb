{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanglongbin/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1/300\n",
      "Epoch 1/1 - Loss: 20.6459\n",
      "Text 2/300\n",
      "Epoch 1/1 - Loss: 20.6464\n",
      "Text 3/300\n",
      "Epoch 1/1 - Loss: 20.6012\n",
      "Text 4/300\n",
      "Epoch 1/1 - Loss: 19.6927\n",
      "Text 5/300\n",
      "Epoch 1/1 - Loss: 19.3878\n",
      "Text 6/300\n",
      "Epoch 1/1 - Loss: 19.6205\n",
      "Text 7/300\n",
      "Epoch 1/1 - Loss: 18.4753\n",
      "Text 8/300\n",
      "Epoch 1/1 - Loss: 18.1159\n",
      "Text 9/300\n",
      "Epoch 1/1 - Loss: 17.7322\n",
      "Text 10/300\n",
      "Epoch 1/1 - Loss: 16.9767\n",
      "Text 11/300\n",
      "Epoch 1/1 - Loss: 16.5071\n",
      "Text 12/300\n",
      "Epoch 1/1 - Loss: 17.7464\n",
      "Text 13/300\n",
      "Epoch 1/1 - Loss: 18.1556\n",
      "Text 14/300\n",
      "Epoch 1/1 - Loss: 15.3395\n",
      "Text 15/300\n",
      "Epoch 1/1 - Loss: 11.8110\n",
      "Text 16/300\n",
      "Epoch 1/1 - Loss: 16.0538\n",
      "Text 17/300\n",
      "Epoch 1/1 - Loss: 16.8147\n",
      "Text 18/300\n",
      "Epoch 1/1 - Loss: 16.8177\n",
      "Text 19/300\n",
      "Epoch 1/1 - Loss: 17.7800\n",
      "Text 20/300\n",
      "Epoch 1/1 - Loss: 16.8130\n",
      "Text 21/300\n",
      "Epoch 1/1 - Loss: 15.1208\n",
      "Text 22/300\n",
      "Epoch 1/1 - Loss: 15.0958\n",
      "Text 23/300\n",
      "Epoch 1/1 - Loss: 17.3478\n",
      "Text 24/300\n",
      "Epoch 1/1 - Loss: 16.4343\n",
      "Text 25/300\n",
      "Epoch 1/1 - Loss: 17.4671\n",
      "Text 26/300\n",
      "Epoch 1/1 - Loss: 18.3356\n",
      "Text 27/300\n",
      "Epoch 1/1 - Loss: 15.4030\n",
      "Text 28/300\n",
      "Epoch 1/1 - Loss: 14.6196\n",
      "Text 29/300\n",
      "Epoch 1/1 - Loss: 19.3867\n",
      "Text 30/300\n",
      "Epoch 1/1 - Loss: 16.9487\n",
      "Text 31/300\n",
      "Epoch 1/1 - Loss: 16.1841\n",
      "Text 32/300\n",
      "Epoch 1/1 - Loss: 16.3595\n",
      "Text 33/300\n",
      "Epoch 1/1 - Loss: 16.2199\n",
      "Text 34/300\n",
      "Epoch 1/1 - Loss: 16.9831\n",
      "Text 35/300\n",
      "Epoch 1/1 - Loss: 17.2578\n",
      "Text 36/300\n",
      "Epoch 1/1 - Loss: 17.2931\n",
      "Text 37/300\n",
      "Epoch 1/1 - Loss: 16.6611\n",
      "Text 38/300\n",
      "Epoch 1/1 - Loss: 15.0058\n",
      "Text 39/300\n",
      "Epoch 1/1 - Loss: 16.6554\n",
      "Text 40/300\n",
      "Epoch 1/1 - Loss: 17.9335\n",
      "Text 41/300\n",
      "Epoch 1/1 - Loss: 18.7084\n",
      "Text 42/300\n",
      "Epoch 1/1 - Loss: 15.5536\n",
      "Text 43/300\n",
      "Epoch 1/1 - Loss: 16.4647\n",
      "Text 44/300\n",
      "Epoch 1/1 - Loss: 15.5740\n",
      "Text 45/300\n",
      "Epoch 1/1 - Loss: 17.3898\n",
      "Text 46/300\n",
      "Epoch 1/1 - Loss: 16.3180\n",
      "Text 47/300\n",
      "Epoch 1/1 - Loss: 16.7285\n",
      "Text 48/300\n",
      "Epoch 1/1 - Loss: 16.8914\n",
      "Text 49/300\n",
      "Epoch 1/1 - Loss: 14.9593\n",
      "Text 50/300\n",
      "Epoch 1/1 - Loss: 15.7406\n",
      "Text 51/300\n",
      "Epoch 1/1 - Loss: 16.1820\n",
      "Text 52/300\n",
      "Epoch 1/1 - Loss: 15.3235\n",
      "Text 53/300\n",
      "Epoch 1/1 - Loss: 15.3506\n",
      "Text 54/300\n",
      "Epoch 1/1 - Loss: 15.4005\n",
      "Text 55/300\n",
      "Epoch 1/1 - Loss: 16.7949\n",
      "Text 56/300\n",
      "Epoch 1/1 - Loss: 15.3146\n",
      "Text 57/300\n",
      "Epoch 1/1 - Loss: 16.3916\n",
      "Text 58/300\n",
      "Epoch 1/1 - Loss: 14.0567\n",
      "Text 59/300\n",
      "Epoch 1/1 - Loss: 14.3549\n",
      "Text 60/300\n",
      "Epoch 1/1 - Loss: 15.8928\n",
      "Text 61/300\n",
      "Epoch 1/1 - Loss: 17.8008\n",
      "Text 62/300\n",
      "Epoch 1/1 - Loss: 14.8901\n",
      "Text 63/300\n",
      "Epoch 1/1 - Loss: 16.3261\n",
      "Text 64/300\n",
      "Epoch 1/1 - Loss: 14.7277\n",
      "Text 65/300\n",
      "Epoch 1/1 - Loss: 16.6615\n",
      "Text 66/300\n",
      "Epoch 1/1 - Loss: 16.8381\n",
      "Text 67/300\n",
      "Epoch 1/1 - Loss: 17.9493\n",
      "Text 68/300\n",
      "Epoch 1/1 - Loss: 15.5318\n",
      "Text 69/300\n",
      "Epoch 1/1 - Loss: 19.0583\n",
      "Text 70/300\n",
      "Epoch 1/1 - Loss: 15.8082\n",
      "Text 71/300\n",
      "Epoch 1/1 - Loss: 18.1571\n",
      "Text 72/300\n",
      "Epoch 1/1 - Loss: 17.5486\n",
      "Text 73/300\n",
      "Epoch 1/1 - Loss: 16.5490\n",
      "Text 74/300\n",
      "Epoch 1/1 - Loss: 14.7333\n",
      "Text 75/300\n",
      "Epoch 1/1 - Loss: 18.4663\n",
      "Text 76/300\n",
      "Epoch 1/1 - Loss: 17.0686\n",
      "Text 77/300\n",
      "Epoch 1/1 - Loss: 15.2622\n",
      "Text 78/300\n",
      "Epoch 1/1 - Loss: 15.7842\n",
      "Text 79/300\n",
      "Epoch 1/1 - Loss: 16.4699\n",
      "Text 80/300\n",
      "Epoch 1/1 - Loss: 12.7031\n",
      "Text 81/300\n",
      "Epoch 1/1 - Loss: 16.2826\n",
      "Text 82/300\n",
      "Epoch 1/1 - Loss: 15.2085\n",
      "Text 83/300\n",
      "Epoch 1/1 - Loss: 14.5420\n",
      "Text 84/300\n",
      "Epoch 1/1 - Loss: 15.9562\n",
      "Text 85/300\n",
      "Epoch 1/1 - Loss: 17.6791\n",
      "Text 86/300\n",
      "Epoch 1/1 - Loss: 14.7311\n",
      "Text 87/300\n",
      "Epoch 1/1 - Loss: 16.4737\n",
      "Text 88/300\n",
      "Epoch 1/1 - Loss: 17.8413\n",
      "Text 89/300\n",
      "Epoch 1/1 - Loss: 17.4269\n",
      "Text 90/300\n",
      "Epoch 1/1 - Loss: 16.9325\n",
      "Text 91/300\n",
      "Epoch 1/1 - Loss: 14.2274\n",
      "Text 92/300\n",
      "Epoch 1/1 - Loss: 14.3233\n",
      "Text 93/300\n",
      "Epoch 1/1 - Loss: 16.3036\n",
      "Text 94/300\n",
      "Epoch 1/1 - Loss: 18.7746\n",
      "Text 95/300\n",
      "Epoch 1/1 - Loss: 17.8205\n",
      "Text 96/300\n",
      "Epoch 1/1 - Loss: 15.6798\n",
      "Text 97/300\n",
      "Epoch 1/1 - Loss: 16.3821\n",
      "Text 98/300\n",
      "Epoch 1/1 - Loss: 14.7817\n",
      "Text 99/300\n",
      "Epoch 1/1 - Loss: 15.8620\n",
      "Text 100/300\n",
      "Epoch 1/1 - Loss: 16.2550\n",
      "Text 101/300\n",
      "Epoch 1/1 - Loss: 16.4551\n",
      "Text 102/300\n",
      "Epoch 1/1 - Loss: 16.6144\n",
      "Text 103/300\n",
      "Epoch 1/1 - Loss: 17.2637\n",
      "Text 104/300\n",
      "Epoch 1/1 - Loss: 15.0642\n",
      "Text 105/300\n",
      "Epoch 1/1 - Loss: 17.2297\n",
      "Text 106/300\n",
      "Epoch 1/1 - Loss: 17.2697\n",
      "Text 107/300\n",
      "Epoch 1/1 - Loss: 16.4360\n",
      "Text 108/300\n",
      "Epoch 1/1 - Loss: 16.2493\n",
      "Text 109/300\n",
      "Epoch 1/1 - Loss: 13.7709\n",
      "Text 110/300\n",
      "Epoch 1/1 - Loss: 19.0628\n",
      "Text 111/300\n",
      "Epoch 1/1 - Loss: 18.7896\n",
      "Text 112/300\n",
      "Epoch 1/1 - Loss: 16.2664\n",
      "Text 113/300\n",
      "Epoch 1/1 - Loss: 15.8647\n",
      "Text 114/300\n",
      "Epoch 1/1 - Loss: 16.2706\n",
      "Text 115/300\n",
      "Epoch 1/1 - Loss: 7.2572\n",
      "Text 116/300\n",
      "Epoch 1/1 - Loss: 15.7148\n",
      "Text 117/300\n",
      "Epoch 1/1 - Loss: 18.0383\n",
      "Text 118/300\n",
      "Epoch 1/1 - Loss: 16.4514\n",
      "Text 119/300\n",
      "Epoch 1/1 - Loss: 16.4053\n",
      "Text 120/300\n",
      "Epoch 1/1 - Loss: 17.2093\n",
      "Text 121/300\n",
      "Epoch 1/1 - Loss: 15.6071\n",
      "Text 122/300\n",
      "Epoch 1/1 - Loss: 14.5860\n",
      "Text 123/300\n",
      "Epoch 1/1 - Loss: 15.5315\n",
      "Text 124/300\n",
      "Epoch 1/1 - Loss: 15.9096\n",
      "Text 125/300\n",
      "Epoch 1/1 - Loss: 16.6117\n",
      "Text 126/300\n",
      "Epoch 1/1 - Loss: 16.1464\n",
      "Text 127/300\n",
      "Epoch 1/1 - Loss: 16.0530\n",
      "Text 128/300\n",
      "Epoch 1/1 - Loss: 16.4954\n",
      "Text 129/300\n",
      "Epoch 1/1 - Loss: 15.6696\n",
      "Text 130/300\n",
      "Epoch 1/1 - Loss: 15.6913\n",
      "Text 131/300\n",
      "Epoch 1/1 - Loss: 10.3973\n",
      "Text 132/300\n",
      "Epoch 1/1 - Loss: 14.2567\n",
      "Text 133/300\n",
      "Epoch 1/1 - Loss: 17.2778\n",
      "Text 134/300\n",
      "Epoch 1/1 - Loss: 16.3556\n",
      "Text 135/300\n",
      "Epoch 1/1 - Loss: 17.2992\n",
      "Text 136/300\n",
      "Epoch 1/1 - Loss: 18.4153\n",
      "Text 137/300\n",
      "Epoch 1/1 - Loss: 16.1692\n",
      "Text 138/300\n",
      "Epoch 1/1 - Loss: 14.9720\n",
      "Text 139/300\n",
      "Epoch 1/1 - Loss: 15.3509\n",
      "Text 140/300\n",
      "Epoch 1/1 - Loss: 15.8592\n",
      "Text 141/300\n",
      "Epoch 1/1 - Loss: 15.7282\n",
      "Text 142/300\n",
      "Epoch 1/1 - Loss: 15.9947\n",
      "Text 143/300\n",
      "Epoch 1/1 - Loss: 16.3429\n",
      "Text 144/300\n",
      "Epoch 1/1 - Loss: 15.9924\n",
      "Text 145/300\n",
      "Epoch 1/1 - Loss: 17.0114\n",
      "Text 146/300\n",
      "Epoch 1/1 - Loss: 19.4936\n",
      "Text 147/300\n",
      "Epoch 1/1 - Loss: 16.6685\n",
      "Text 148/300\n",
      "Epoch 1/1 - Loss: 10.5296\n",
      "Text 149/300\n",
      "Epoch 1/1 - Loss: 15.8877\n",
      "Text 150/300\n",
      "Epoch 1/1 - Loss: 17.2854\n",
      "Text 151/300\n",
      "Epoch 1/1 - Loss: 17.0084\n",
      "Text 152/300\n",
      "Epoch 1/1 - Loss: 15.2844\n",
      "Text 153/300\n",
      "Epoch 1/1 - Loss: 18.1841\n",
      "Text 154/300\n",
      "Epoch 1/1 - Loss: 16.6090\n",
      "Text 155/300\n",
      "Epoch 1/1 - Loss: 17.2645\n",
      "Text 156/300\n",
      "Epoch 1/1 - Loss: 14.9036\n",
      "Text 157/300\n",
      "Epoch 1/1 - Loss: 16.3817\n",
      "Text 158/300\n",
      "Epoch 1/1 - Loss: 14.0946\n",
      "Text 159/300\n",
      "Epoch 1/1 - Loss: 17.4788\n",
      "Text 160/300\n",
      "Epoch 1/1 - Loss: 15.4380\n",
      "Text 161/300\n",
      "Epoch 1/1 - Loss: 14.9429\n",
      "Text 162/300\n",
      "Epoch 1/1 - Loss: 17.4010\n",
      "Text 163/300\n",
      "Epoch 1/1 - Loss: 16.2646\n",
      "Text 164/300\n",
      "Epoch 1/1 - Loss: 17.0298\n",
      "Text 165/300\n",
      "Epoch 1/1 - Loss: 13.3620\n",
      "Text 166/300\n",
      "Epoch 1/1 - Loss: 16.0629\n",
      "Text 167/300\n",
      "Epoch 1/1 - Loss: 16.6093\n",
      "Text 168/300\n",
      "Epoch 1/1 - Loss: 15.9558\n",
      "Text 169/300\n",
      "Epoch 1/1 - Loss: 16.0214\n",
      "Text 170/300\n",
      "Epoch 1/1 - Loss: 8.0009\n",
      "Text 171/300\n",
      "Epoch 1/1 - Loss: 15.2894\n",
      "Text 172/300\n",
      "Epoch 1/1 - Loss: 15.2073\n",
      "Text 173/300\n",
      "Epoch 1/1 - Loss: 15.6781\n",
      "Text 174/300\n",
      "Epoch 1/1 - Loss: 16.1479\n",
      "Text 175/300\n",
      "Epoch 1/1 - Loss: 9.7756\n",
      "Text 176/300\n",
      "Epoch 1/1 - Loss: 16.3433\n",
      "Text 177/300\n",
      "Epoch 1/1 - Loss: 17.0824\n",
      "Text 178/300\n",
      "Epoch 1/1 - Loss: 14.5347\n",
      "Text 179/300\n",
      "Epoch 1/1 - Loss: 16.2318\n",
      "Text 180/300\n",
      "Epoch 1/1 - Loss: 15.1946\n",
      "Text 181/300\n",
      "Epoch 1/1 - Loss: 15.3814\n",
      "Text 182/300\n",
      "Epoch 1/1 - Loss: 14.7297\n",
      "Text 183/300\n",
      "Epoch 1/1 - Loss: 15.8119\n",
      "Text 184/300\n",
      "Epoch 1/1 - Loss: 15.8203\n",
      "Text 185/300\n",
      "Epoch 1/1 - Loss: 14.7430\n",
      "Text 186/300\n",
      "Epoch 1/1 - Loss: 16.7815\n",
      "Text 187/300\n",
      "Epoch 1/1 - Loss: 15.6875\n",
      "Text 188/300\n",
      "Epoch 1/1 - Loss: 15.5687\n",
      "Text 189/300\n",
      "Epoch 1/1 - Loss: 16.1572\n",
      "Text 190/300\n",
      "Epoch 1/1 - Loss: 15.5274\n",
      "Text 191/300\n",
      "Epoch 1/1 - Loss: 16.6395\n",
      "Text 192/300\n",
      "Epoch 1/1 - Loss: 16.7053\n",
      "Text 193/300\n",
      "Epoch 1/1 - Loss: 12.3319\n",
      "Text 194/300\n",
      "Epoch 1/1 - Loss: 17.4787\n",
      "Text 195/300\n",
      "Epoch 1/1 - Loss: 16.8268\n",
      "Text 196/300\n",
      "Epoch 1/1 - Loss: 16.9514\n",
      "Text 197/300\n",
      "Epoch 1/1 - Loss: 15.8891\n",
      "Text 198/300\n",
      "Epoch 1/1 - Loss: 17.3565\n",
      "Text 199/300\n",
      "Epoch 1/1 - Loss: 15.4780\n",
      "Text 200/300\n",
      "Epoch 1/1 - Loss: 15.7842\n",
      "Text 201/300\n",
      "Epoch 1/1 - Loss: 14.9544\n",
      "Text 202/300\n",
      "Epoch 1/1 - Loss: 15.8256\n",
      "Text 203/300\n",
      "Epoch 1/1 - Loss: 16.8026\n",
      "Text 204/300\n",
      "Epoch 1/1 - Loss: 15.1014\n",
      "Text 205/300\n",
      "Epoch 1/1 - Loss: 14.1086\n",
      "Text 206/300\n",
      "Epoch 1/1 - Loss: 14.2742\n",
      "Text 207/300\n",
      "Epoch 1/1 - Loss: 16.0932\n",
      "Text 208/300\n",
      "Epoch 1/1 - Loss: 14.1966\n",
      "Text 209/300\n",
      "Epoch 1/1 - Loss: 17.1078\n",
      "Text 210/300\n",
      "Epoch 1/1 - Loss: 17.2794\n",
      "Text 211/300\n",
      "Epoch 1/1 - Loss: 17.7829\n",
      "Text 212/300\n",
      "Epoch 1/1 - Loss: 17.2812\n",
      "Text 213/300\n",
      "Epoch 1/1 - Loss: 17.5449\n",
      "Text 214/300\n",
      "Epoch 1/1 - Loss: 16.3481\n",
      "Text 215/300\n",
      "Epoch 1/1 - Loss: 17.2277\n",
      "Text 216/300\n",
      "Epoch 1/1 - Loss: 14.9414\n",
      "Text 217/300\n",
      "Epoch 1/1 - Loss: 15.7141\n",
      "Text 218/300\n",
      "Epoch 1/1 - Loss: 15.3648\n",
      "Text 219/300\n",
      "Epoch 1/1 - Loss: 14.5487\n",
      "Text 220/300\n",
      "Epoch 1/1 - Loss: 16.0179\n",
      "Text 221/300\n",
      "Epoch 1/1 - Loss: 17.0317\n",
      "Text 222/300\n",
      "Epoch 1/1 - Loss: 15.9758\n",
      "Text 223/300\n",
      "Epoch 1/1 - Loss: 14.9026\n",
      "Text 224/300\n",
      "Epoch 1/1 - Loss: 14.9293\n",
      "Text 225/300\n",
      "Epoch 1/1 - Loss: 18.0252\n",
      "Text 226/300\n",
      "Epoch 1/1 - Loss: 16.1363\n",
      "Text 227/300\n",
      "Epoch 1/1 - Loss: 17.5341\n",
      "Text 228/300\n",
      "Epoch 1/1 - Loss: 16.1095\n",
      "Text 229/300\n",
      "Epoch 1/1 - Loss: 15.3881\n",
      "Text 230/300\n",
      "Epoch 1/1 - Loss: 15.6503\n",
      "Text 231/300\n",
      "Epoch 1/1 - Loss: 15.9739\n",
      "Text 232/300\n",
      "Epoch 1/1 - Loss: 16.7934\n",
      "Text 233/300\n",
      "Epoch 1/1 - Loss: 16.3583\n",
      "Text 234/300\n",
      "Epoch 1/1 - Loss: 16.3440\n",
      "Text 235/300\n",
      "Epoch 1/1 - Loss: 16.0640\n",
      "Text 236/300\n",
      "Epoch 1/1 - Loss: 15.5455\n",
      "Text 237/300\n",
      "Epoch 1/1 - Loss: 14.2526\n",
      "Text 238/300\n",
      "Epoch 1/1 - Loss: 15.5524\n",
      "Text 239/300\n",
      "Epoch 1/1 - Loss: 14.8047\n",
      "Text 240/300\n",
      "Epoch 1/1 - Loss: 15.5373\n",
      "Text 241/300\n",
      "Epoch 1/1 - Loss: 14.2084\n",
      "Text 242/300\n",
      "Epoch 1/1 - Loss: 15.8750\n",
      "Text 243/300\n",
      "Epoch 1/1 - Loss: 15.9413\n",
      "Text 244/300\n",
      "Epoch 1/1 - Loss: 16.7672\n",
      "Text 245/300\n",
      "Epoch 1/1 - Loss: 16.7652\n",
      "Text 246/300\n",
      "Epoch 1/1 - Loss: 15.0132\n",
      "Text 247/300\n",
      "Epoch 1/1 - Loss: 15.2646\n",
      "Text 248/300\n",
      "Epoch 1/1 - Loss: 15.8386\n",
      "Text 249/300\n",
      "Epoch 1/1 - Loss: 18.3197\n",
      "Text 250/300\n",
      "Epoch 1/1 - Loss: 15.0139\n",
      "Text 251/300\n",
      "Epoch 1/1 - Loss: 16.0949\n",
      "Text 252/300\n",
      "Epoch 1/1 - Loss: 15.6656\n",
      "Text 253/300\n",
      "Epoch 1/1 - Loss: 16.4032\n",
      "Text 254/300\n",
      "Epoch 1/1 - Loss: 15.9455\n",
      "Text 255/300\n",
      "Epoch 1/1 - Loss: 16.9161\n",
      "Text 256/300\n",
      "Epoch 1/1 - Loss: 15.1664\n",
      "Text 257/300\n",
      "Epoch 1/1 - Loss: 16.4641\n",
      "Text 258/300\n",
      "Epoch 1/1 - Loss: 17.0548\n",
      "Text 259/300\n",
      "Epoch 1/1 - Loss: 17.1588\n",
      "Text 260/300\n",
      "Epoch 1/1 - Loss: 17.6451\n",
      "Text 261/300\n",
      "Epoch 1/1 - Loss: 15.8690\n",
      "Text 262/300\n",
      "Epoch 1/1 - Loss: 17.0117\n",
      "Text 263/300\n",
      "Epoch 1/1 - Loss: 18.6366\n",
      "Text 264/300\n",
      "Epoch 1/1 - Loss: 14.3539\n",
      "Text 265/300\n",
      "Epoch 1/1 - Loss: 17.3082\n",
      "Text 266/300\n",
      "Epoch 1/1 - Loss: 18.2571\n",
      "Text 267/300\n",
      "Epoch 1/1 - Loss: 16.6245\n",
      "Text 268/300\n",
      "Epoch 1/1 - Loss: 18.7180\n",
      "Text 269/300\n",
      "Epoch 1/1 - Loss: 16.4697\n",
      "Text 270/300\n",
      "Epoch 1/1 - Loss: 18.2435\n",
      "Text 271/300\n",
      "Epoch 1/1 - Loss: 16.9154\n",
      "Text 272/300\n",
      "Epoch 1/1 - Loss: 16.4020\n",
      "Text 273/300\n",
      "Epoch 1/1 - Loss: 15.7330\n",
      "Text 274/300\n",
      "Epoch 1/1 - Loss: 18.1194\n",
      "Text 275/300\n",
      "Epoch 1/1 - Loss: 16.1212\n",
      "Text 276/300\n",
      "Epoch 1/1 - Loss: 17.6235\n",
      "Text 277/300\n",
      "Epoch 1/1 - Loss: 16.6946\n",
      "Text 278/300\n",
      "Epoch 1/1 - Loss: 17.5549\n",
      "Text 279/300\n",
      "Epoch 1/1 - Loss: 16.0410\n",
      "Text 280/300\n",
      "Epoch 1/1 - Loss: 15.7644\n",
      "Text 281/300\n",
      "Epoch 1/1 - Loss: 16.6360\n",
      "Text 282/300\n",
      "Epoch 1/1 - Loss: 16.2774\n",
      "Text 283/300\n",
      "Epoch 1/1 - Loss: 16.0129\n",
      "Text 284/300\n",
      "Epoch 1/1 - Loss: 14.1318\n",
      "Text 285/300\n",
      "Epoch 1/1 - Loss: 16.9450\n",
      "Text 286/300\n",
      "Epoch 1/1 - Loss: 14.5996\n",
      "Text 287/300\n",
      "Epoch 1/1 - Loss: 14.4751\n",
      "Text 288/300\n",
      "Epoch 1/1 - Loss: 17.2567\n",
      "Text 289/300\n",
      "Epoch 1/1 - Loss: 15.8687\n",
      "Text 290/300\n",
      "Epoch 1/1 - Loss: 16.0393\n",
      "Text 291/300\n",
      "Epoch 1/1 - Loss: 15.1006\n",
      "Text 292/300\n",
      "Epoch 1/1 - Loss: 15.7864\n",
      "Text 293/300\n",
      "Epoch 1/1 - Loss: 18.1061\n",
      "Text 294/300\n",
      "Epoch 1/1 - Loss: 16.6849\n",
      "Text 295/300\n",
      "Epoch 1/1 - Loss: 15.4874\n",
      "Text 296/300\n",
      "Epoch 1/1 - Loss: 17.5892\n",
      "Text 297/300\n",
      "Epoch 1/1 - Loss: 16.0075\n",
      "Text 298/300\n",
      "Epoch 1/1 - Loss: 16.2938\n",
      "Text 299/300\n",
      "Epoch 1/1 - Loss: 16.0016\n",
      "Text 300/300\n",
      "Epoch 1/1 - Loss: 17.7395\n",
      "[['coronavirus', 1.7320508050033823e-05], ['acids', 0.09797880053520203], ['risk', 0.10326455533504486], ['levels', 0.11198123544454575], ['pair', 0.11583508551120758], ['stratified', 0.12138685584068298], ['response', 0.12665817141532898], ['responses', 0.13310670852661133], ['female', 0.14507219195365906], ['ability', 0.15487238764762878], ['food', 0.1574103832244873], ['group', 0.15902672708034515], ['new', 0.16336798667907715], ['exchange', 0.16366735100746155], ['stress', 0.1769995540380478], ['series', 0.17711392045021057], ['upon', 0.17754480242729187], ['pathway', 0.17784899473190308], ['impacted', 0.18154002726078033], ['partial', 0.1914934664964676], ['organization', 0.19157187640666962], ['quantity', 0.19207395613193512], ['dominant', 0.19628585875034332], ['fragmentation', 0.19900070130825043], ['great', 0.1993928700685501], ['square', 0.20200444757938385], ['O', 0.20253878831863403], ['glycan', 0.2042553573846817], ['protein', 0.20705203711986542], ['Lewis', 0.20790013670921326], ['during', 0.20946377515792847], ['side', 0.2123546153306961], ['communicating', 0.21412873268127441], ['soon', 0.21498851478099823], ['induction', 0.21824567019939423], ['mediated', 0.21872904896736145], ['cycle', 0.2192685902118683], ['sequencing', 0.22062742710113525], ['placed', 0.2208300679922104], ['moderate', 0.22216708958148956], ['immunodeficiency', 0.2251686155796051], ['aspirin', 0.22602233290672302], ['Two', 0.22633297741413116], ['yellow', 0.22798040509223938], ['incomplete', 0.22804959118366241], ['formation', 0.22858871519565582], ['formed', 0.23781642317771912], ['Blood', 0.23994125425815582], ['dopaminergic', 0.24187609553337097], ['health', 0.24187810719013214], ['prevalence', 0.24511875212192535], ['involving', 0.24872934818267822], ['as', 0.2522604763507843], ['Fiber', 0.2556275427341461], ['sex', 0.2584850490093231], ['contralateral', 0.26441526412963867], ['linear', 0.2644535005092621], ['clinical', 0.2649939954280853], ['history', 0.2655167281627655], ['consumption', 0.2662789821624756], ['separation', 0.26674896478652954], ['molecular', 0.2668302357196808], ['extent', 0.2670251429080963], ['activities', 0.2686610817909241], ['basal', 0.27055251598358154], ['neutral', 0.27096840739250183], ['horizontal', 0.2720930874347687], ['groups', 0.2735314667224884], ['location', 0.2752835154533386], ['chain', 0.2759316563606262], ['from', 0.2761678695678711], ['fatigue', 0.27641215920448303], ['Clinical', 0.2768794298171997], ['on', 0.2775055170059204], ['processes', 0.2778101861476898], ['killing', 0.2782118320465088], ['influenzae', 0.27849510312080383], ['observation', 0.2797205448150635], ['postsynaptic', 0.2799604833126068], ['water', 0.2803069055080414], ['controlling', 0.2810727655887604], ['delay', 0.2823849022388458], ['H', 0.2835826575756073], ['with', 0.2848741114139557], ['insecticide', 0.2855938971042633], ['structural', 0.28571614623069763], ['Areas', 0.286632776260376], ['plan', 0.2870204448699951], ['episodes', 0.28899452090263367], ['inflammatory', 0.2899966835975647], ['temporal', 0.29148349165916443], ['Respiratory', 0.29164987802505493], ['pattern', 0.2917417883872986], ['a', 0.29311826825141907], ['infection', 0.295091450214386], ['intracellular', 0.29557037353515625], ['artery', 0.2977469265460968], ['human', 0.2988525629043579], ['Food', 0.29891347885131836], ['reverse', 0.299426406621933]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Biomedical Entities 文件夹目录\n",
    "OwlDirPath = \"/home/tanglongbin/NLP/owl\"\n",
    "# owl文件数量\n",
    "OwlFileNum = 3\n",
    "# 分词文件夹目录\n",
    "TokensDirPath = \"/home/tanglongbin/NLP/nltk_text\"\n",
    "# 作为词汇表的Tokens文件数量\n",
    "TokensFileNum = 50\n",
    "# 词汇量\n",
    "VocabSize = 10000\n",
    "\n",
    "# 文章目录（split_text目录）\n",
    "TextDirPath = \"/home/tanglongbin/NLP/split_text\"\n",
    "# 用于训练的文章数量\n",
    "TextNum = 300\n",
    "# 每篇文章训练的次数\n",
    "EpochsNum = 1\n",
    "# 中心词一侧Positive单词的数量\n",
    "WindowSize = 5\n",
    "# Negative单词的数量\n",
    "NegaNum = 2\n",
    "# 词向量Feature维度\n",
    "EmbedSize = 300\n",
    "\n",
    "# 单次训练使用的数量（可更具内存/显卡负载进行调整）\n",
    "Batch_Size = 512\n",
    "# 学习速率（步长）\n",
    "LearningRate = 0.001\n",
    "\n",
    "\n",
    "\n",
    "# 获取指定文件夹内的所有文件的绝对路径\n",
    "# return type: List\n",
    "def GetFilePath(DirPath):\n",
    "    \n",
    "    Res = []\n",
    "    for FilePath, DirNames, FileNames in os.walk(DirPath):\n",
    "        for FileName in FileNames:\n",
    "            str_tmp = os.path.join(FilePath, FileName)\n",
    "            Res.append(str_tmp)\n",
    "            \n",
    "    return Res\n",
    "\n",
    "\n",
    "# 获取Vocabulary\n",
    "# 返回设定词汇量的vocab字典\n",
    "def GetVocab():\n",
    "    TmpList = []\n",
    "    owl_list = []\n",
    "    TokenList = []\n",
    "    OwlFiles = GetFilePath(OwlDirPath)\n",
    "    for i in range(OwlFileNum):\n",
    "        with open(OwlFiles[i], 'r') as f:\n",
    "            S = f.readline()\n",
    "            # String to List\n",
    "            L = eval(S)\n",
    "            TmpList += L\n",
    "    TmpList = [x[0] for x in TmpList]\n",
    "    for item in TmpList:\n",
    "        owl_list += item.split()\n",
    "    \n",
    "    TmpList = []\n",
    "    TokensFiles = GetFilePath(TokensDirPath)\n",
    "    for i in range(TokensFileNum):\n",
    "        with open(TokensFiles[i], 'r') as f:\n",
    "            S = f.readline()\n",
    "            # String to List\n",
    "            L = eval(S)\n",
    "            TmpList += L\n",
    "    \n",
    "    for item in TmpList:\n",
    "        if item in owl_list:\n",
    "            TokenList.append(item) \n",
    "    \n",
    "    # 获取频率最高的VocabSize-1个单词，并将剩下的单词归类为'<unk>'\n",
    "    Vocab = dict(Counter(TokenList).most_common(VocabSize-1))\n",
    "    Vocab[\"<unk>\"] = len(TokenList) - np.sum(list(Vocab.values()))\n",
    "    return Vocab\n",
    "\n",
    "\n",
    "# 对Vocab里的单词进行编码和词频处理\n",
    "def AnalyzeVocab(Vocab):\n",
    "    # encoder & decoder\n",
    "    idx_to_word = [word for word in Vocab.keys()] \n",
    "    word_to_idx = {word:i for i, word in enumerate(idx_to_word)}\n",
    "    \n",
    "    # 词频变换 \n",
    "    word_counts = np.array([count for count in Vocab.values()], dtype=np.float32)\n",
    "    word_freqs = word_counts / np.sum(word_counts)\n",
    "    word_freqs = word_freqs ** (3./4.)\n",
    "    word_freqs = word_freqs / np.sum(word_freqs)  # 用来做 negative sampling\n",
    "    \n",
    "    return word_to_idx, idx_to_word, word_freqs\n",
    "\n",
    "\n",
    "# 创建Dataset, 并对文章进行编码\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, text, word_to_idx, idx_to_word, word_freqs):\n",
    "        super(Dataset, self).__init__()\n",
    "        # 编码文章\n",
    "        self.text_encoded = [word_to_idx.get(word, word_to_idx[\"<unk>\"]) for word in text]\n",
    "        self.word_to_idx = word_to_idx \n",
    "        self.idx_to_word = idx_to_word\n",
    "        # 转换为Tensor以便于GPU训练\n",
    "        self.text_encoded = torch.Tensor(self.text_encoded).long()\n",
    "        self.word_freqs = torch.Tensor(word_freqs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_encoded)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ''' 这个function返回以下数据用于训练\n",
    "            - 中心词\n",
    "            - 这个单词附近的(positive)单词\n",
    "            - 随机采样的K个单词作为negative sample\n",
    "        '''\n",
    "        center_word = self.text_encoded[idx] \n",
    "        pos_indices = list(range(idx-WindowSize, idx)) + list(range(idx + 1, idx + WindowSize + 1))\n",
    "        pos_indices = [i % len(self.text_encoded) for i in pos_indices]\n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "        # replacement=False无放回的取\n",
    "        neg_words = torch.multinomial(self.word_freqs, (NegaNum + 1) * pos_words.shape[0], replacement=False)\n",
    "        # Neg_words 与 pos_words 取差集，确保 neg_words 中不包含 pos_words\n",
    "        neg_words = np.setdiff1d(neg_words.numpy(), pos_words.numpy())\n",
    "        neg_words = neg_words[:NegaNum * pos_words.shape[0]]\n",
    "        neg_words = torch.Tensor(neg_words)\n",
    "    \n",
    "        return center_word, pos_words, neg_words \n",
    "\n",
    "\n",
    "# Skip-Gram Model\n",
    "class EmbeddingModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        # 模型建立 & 初始化\n",
    "        self.embed = torch.nn.Embedding(vocab_size, embed_size)\n",
    "        initrange = 0.5 / embed_size\n",
    "        self.embed.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        '''\n",
    "        input_labels: 中心词,         [batch_size]\n",
    "        pos_labels: 中心词周围词       [batch_size * (WindowSize * 2)]\n",
    "        neg_labelss: 中心词负采样单词  [batch_size, (WindowSize * 2 * NegaNum)]\n",
    "        return: loss, 返回loss        [batch_size]\n",
    "        '''\n",
    "        \n",
    "        # 编码\n",
    "        input_embedding = self.embed(input_labels)\n",
    "        input_embedding = input_embedding.unsqueeze(dim = 2)\n",
    "        pos_embedding = self.embed(pos_labels)\n",
    "        neg_embedding = self.embed(neg_labels)\n",
    "        \n",
    "\n",
    "        # torch.bmm()为batch间的矩阵相乘（b,n.m)*(b,m,p)=(b,n,p)\n",
    "        loss_pos = torch.bmm(pos_embedding, input_embedding).squeeze()\n",
    "        loss_neg = torch.bmm(neg_embedding, -input_embedding).squeeze()\n",
    "        \n",
    "        # loss计算\n",
    "        loss_pos = loss_pos.sigmoid().log().sum(dim = 1)\n",
    "        loss_neg = loss_neg.sigmoid().log().sum(dim = 1)\n",
    "        loss = (loss_pos + loss_neg).mean(dim = 0)\n",
    "        return -loss\n",
    "    \n",
    "    def Embeddings(self):\n",
    "        return self.embed.weight.data.cpu().numpy()\n",
    "\n",
    "\n",
    "# 返回与 Input_word 最相似的 Word_num 个单词\n",
    "# 返回一个二维 List[Word_num][1]\n",
    "# List[Word_num][0] 为单词，List[Word_num][1] 为相似度\n",
    "def CosineSimilarity(Model, word_to_idx, idx_to_word, Input_word, Word_num):\n",
    "    # 创建 Model Copy\n",
    "    W = Model.embed.weight.data.clone()\n",
    "    # 单位化\n",
    "    norm = W.norm(dim = 1).unsqueeze(dim = 1)\n",
    "    W = W/norm\n",
    "    \n",
    "    # 获取 Input_word 词向量\n",
    "    ids = [word_to_idx.get(Input_word, word_to_idx[\"<unk>\"])]\n",
    "    x = W[ids]\n",
    "    # 计算 similarity\n",
    "    similarity = torch.mm(x, W.T)\n",
    "    \n",
    "    topk = (-similarity[0,:]).argsort()[:Word_num]\n",
    "    Res = [[idx_to_word[j.item()], similarity[0][j.item()].item()] for j in topk]\n",
    "    \n",
    "    return Res\n",
    "\n",
    "\n",
    "# 返回与 Input_word 距离最小的 Word_num 个单词\n",
    "# 返回一个二维 List[Word_num][1]\n",
    "# List[Word_num][0] 为单词，List[Word_num][1] 为距离\n",
    "def FindNearest(Model, word_to_idx, idx_to_word, Input_word, Word_num):\n",
    "    # 创建 Model Copy\n",
    "    W = Model.embed.weight.data.clone()\n",
    "    # 获取 Input_word 词向量\n",
    "    ids = [word_to_idx.get(Input_word, word_to_idx[\"<unk>\"])]\n",
    "    x = W[ids]\n",
    "    \n",
    "    # 计算所有30000个embedding向量与传入单词embedding向量的相似度距离\n",
    "    Distance = torch.nn.PairwiseDistance(p=2)\n",
    "    cos_dis = Distance(x,W)\n",
    "    topk = cos_dis.argsort()[:Word_num]\n",
    "    Res = [[idx_to_word[j.item()], cos_dis[j.item()].item()] for j in topk]\n",
    "    \n",
    "    return Res\n",
    "\n",
    "\n",
    "def  StartTraining():\n",
    "    \n",
    "    # 获取Vocab\n",
    "    Vocab = GetVocab()\n",
    "    # 获得Vocab分析数据\n",
    "    word_to_idx, idx_to_word, word_freqs = AnalyzeVocab(Vocab)\n",
    "    # print(word_to_idx, idx_to_word, word_freqs)\n",
    "    # return\n",
    "    # 创建模型(使用Adam算法)\n",
    "    Model = EmbeddingModel(VocabSize, EmbedSize)\n",
    "    if torch.cuda.is_available():\n",
    "        # 若支持cuda加速则使用GPU训练\n",
    "        Model = Model.cuda()\n",
    "    Optimizer = torch.optim.Adam(Model.parameters(), lr = LearningRate)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 读取Text\n",
    "    TextFiles = GetFilePath(TextDirPath)\n",
    "    for i in range(TextNum):\n",
    "        with open(TextFiles[i]) as f:\n",
    "            S = f.readline()\n",
    "            # String to List\n",
    "            Text = eval(S)\n",
    "            \n",
    "            # 创建DataLoader\n",
    "            dataset = Dataset(Text, word_to_idx, idx_to_word, word_freqs)\n",
    "            dataloader = torch.utils.data.DataLoader(dataset, batch_size = Batch_Size, shuffle = True, num_workers = 2)\n",
    "            \n",
    "            print(f'Text {i+1}/{TextNum}')\n",
    "            # StartTraining\n",
    "            for j in range(EpochsNum):\n",
    "                for k, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "                    # 转换为 long int 类型\n",
    "                    input_labels = input_labels.long()\n",
    "                    pos_labels = pos_labels.long()\n",
    "                    neg_labels = neg_labels.long()\n",
    "                    if torch.cuda.is_available(): \n",
    "                        # 若支持cuda加速则使用GPU训练\n",
    "                        input_labels = input_labels.cuda()\n",
    "                        pos_labels = pos_labels.cuda()\n",
    "                        neg_labels = neg_labels.cuda()\n",
    "\n",
    "                    loss = Model(input_labels, pos_labels, neg_labels)\n",
    "                    Optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    Optimizer.step()\n",
    "        \n",
    "                print(f'Epoch {j+1}/{EpochsNum} - Loss: {loss.item():.4f}')\n",
    "    print(FindNearest(Model, word_to_idx, idx_to_word, \"coronavirus\", 100))\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # 开始训练\n",
    "    StartTraining()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
