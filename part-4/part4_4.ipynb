{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanglongbin/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1/300\n",
      "Epoch 1/1 - Loss: 20.6221\n",
      "Text 2/300\n",
      "Epoch 1/1 - Loss: 20.6481\n",
      "Text 3/300\n",
      "Epoch 1/1 - Loss: 20.6103\n",
      "Text 4/300\n",
      "Epoch 1/1 - Loss: 19.7486\n",
      "Text 5/300\n",
      "Epoch 1/1 - Loss: 19.3783\n",
      "Text 6/300\n",
      "Epoch 1/1 - Loss: 19.4908\n",
      "Text 7/300\n",
      "Epoch 1/1 - Loss: 18.3132\n",
      "Text 8/300\n",
      "Epoch 1/1 - Loss: 18.2009\n",
      "Text 9/300\n",
      "Epoch 1/1 - Loss: 17.9002\n",
      "Text 10/300\n",
      "Epoch 1/1 - Loss: 16.9288\n",
      "Text 11/300\n",
      "Epoch 1/1 - Loss: 17.0980\n",
      "Text 12/300\n",
      "Epoch 1/1 - Loss: 17.3648\n",
      "Text 13/300\n",
      "Epoch 1/1 - Loss: 17.6092\n",
      "Text 14/300\n",
      "Epoch 1/1 - Loss: 15.7271\n",
      "Text 15/300\n",
      "Epoch 1/1 - Loss: 11.7755\n",
      "Text 16/300\n",
      "Epoch 1/1 - Loss: 15.9462\n",
      "Text 17/300\n",
      "Epoch 1/1 - Loss: 17.1530\n",
      "Text 18/300\n",
      "Epoch 1/1 - Loss: 16.9288\n",
      "Text 19/300\n",
      "Epoch 1/1 - Loss: 17.6384\n",
      "Text 20/300\n",
      "Epoch 1/1 - Loss: 17.0898\n",
      "Text 21/300\n",
      "Epoch 1/1 - Loss: 15.4433\n",
      "Text 22/300\n",
      "Epoch 1/1 - Loss: 15.0992\n",
      "Text 23/300\n",
      "Epoch 1/1 - Loss: 16.4905\n",
      "Text 24/300\n",
      "Epoch 1/1 - Loss: 16.2897\n",
      "Text 25/300\n",
      "Epoch 1/1 - Loss: 17.3262\n",
      "Text 26/300\n",
      "Epoch 1/1 - Loss: 18.9956\n",
      "Text 27/300\n",
      "Epoch 1/1 - Loss: 18.4705\n",
      "Text 28/300\n",
      "Epoch 1/1 - Loss: 14.7603\n",
      "Text 29/300\n",
      "Epoch 1/1 - Loss: 16.1758\n",
      "Text 30/300\n",
      "Epoch 1/1 - Loss: 16.4533\n",
      "Text 31/300\n",
      "Epoch 1/1 - Loss: 16.3898\n",
      "Text 32/300\n",
      "Epoch 1/1 - Loss: 15.6500\n",
      "Text 33/300\n",
      "Epoch 1/1 - Loss: 15.4927\n",
      "Text 34/300\n",
      "Epoch 1/1 - Loss: 16.7363\n",
      "Text 35/300\n",
      "Epoch 1/1 - Loss: 17.8666\n",
      "Text 36/300\n",
      "Epoch 1/1 - Loss: 16.8517\n",
      "Text 37/300\n",
      "Epoch 1/1 - Loss: 16.8269\n",
      "Text 38/300\n",
      "Epoch 1/1 - Loss: 15.2386\n",
      "Text 39/300\n",
      "Epoch 1/1 - Loss: 15.4490\n",
      "Text 40/300\n",
      "Epoch 1/1 - Loss: 17.6758\n",
      "Text 41/300\n",
      "Epoch 1/1 - Loss: 19.7694\n",
      "Text 42/300\n",
      "Epoch 1/1 - Loss: 14.2429\n",
      "Text 43/300\n",
      "Epoch 1/1 - Loss: 16.6695\n",
      "Text 44/300\n",
      "Epoch 1/1 - Loss: 15.9564\n",
      "Text 45/300\n",
      "Epoch 1/1 - Loss: 17.0119\n",
      "Text 46/300\n",
      "Epoch 1/1 - Loss: 16.0361\n",
      "Text 47/300\n",
      "Epoch 1/1 - Loss: 16.3417\n",
      "Text 48/300\n",
      "Epoch 1/1 - Loss: 17.6742\n",
      "Text 49/300\n",
      "Epoch 1/1 - Loss: 14.6657\n",
      "Text 50/300\n",
      "Epoch 1/1 - Loss: 16.1898\n",
      "Text 51/300\n",
      "Epoch 1/1 - Loss: 16.3719\n",
      "Text 52/300\n",
      "Epoch 1/1 - Loss: 15.3696\n",
      "Text 53/300\n",
      "Epoch 1/1 - Loss: 15.3803\n",
      "Text 54/300\n",
      "Epoch 1/1 - Loss: 15.0109\n",
      "Text 55/300\n",
      "Epoch 1/1 - Loss: 18.1679\n",
      "Text 56/300\n",
      "Epoch 1/1 - Loss: 15.4340\n",
      "Text 57/300\n",
      "Epoch 1/1 - Loss: 16.9835\n",
      "Text 58/300\n",
      "Epoch 1/1 - Loss: 12.8862\n",
      "Text 59/300\n",
      "Epoch 1/1 - Loss: 13.4197\n",
      "Text 60/300\n",
      "Epoch 1/1 - Loss: 15.9107\n",
      "Text 61/300\n",
      "Epoch 1/1 - Loss: 17.8689\n",
      "Text 62/300\n",
      "Epoch 1/1 - Loss: 14.9203\n",
      "Text 63/300\n",
      "Epoch 1/1 - Loss: 16.9588\n",
      "Text 64/300\n",
      "Epoch 1/1 - Loss: 14.6175\n",
      "Text 65/300\n",
      "Epoch 1/1 - Loss: 17.2265\n",
      "Text 66/300\n",
      "Epoch 1/1 - Loss: 17.1837\n",
      "Text 67/300\n",
      "Epoch 1/1 - Loss: 17.8832\n",
      "Text 68/300\n",
      "Epoch 1/1 - Loss: 15.6468\n",
      "Text 69/300\n",
      "Epoch 1/1 - Loss: 16.4146\n",
      "Text 70/300\n",
      "Epoch 1/1 - Loss: 16.0928\n",
      "Text 71/300\n",
      "Epoch 1/1 - Loss: 18.1073\n",
      "Text 72/300\n",
      "Epoch 1/1 - Loss: 17.3682\n",
      "Text 73/300\n",
      "Epoch 1/1 - Loss: 16.4372\n",
      "Text 74/300\n",
      "Epoch 1/1 - Loss: 15.1831\n",
      "Text 75/300\n",
      "Epoch 1/1 - Loss: 16.2470\n",
      "Text 76/300\n",
      "Epoch 1/1 - Loss: 16.9980\n",
      "Text 77/300\n",
      "Epoch 1/1 - Loss: 15.4466\n",
      "Text 78/300\n",
      "Epoch 1/1 - Loss: 15.6856\n",
      "Text 79/300\n",
      "Epoch 1/1 - Loss: 16.4814\n",
      "Text 80/300\n",
      "Epoch 1/1 - Loss: 12.5258\n",
      "Text 81/300\n",
      "Epoch 1/1 - Loss: 16.2352\n",
      "Text 82/300\n",
      "Epoch 1/1 - Loss: 16.1669\n",
      "Text 83/300\n",
      "Epoch 1/1 - Loss: 15.7618\n",
      "Text 84/300\n",
      "Epoch 1/1 - Loss: 16.3993\n",
      "Text 85/300\n",
      "Epoch 1/1 - Loss: 17.7176\n",
      "Text 86/300\n",
      "Epoch 1/1 - Loss: 16.1717\n",
      "Text 87/300\n",
      "Epoch 1/1 - Loss: 16.4855\n",
      "Text 88/300\n",
      "Epoch 1/1 - Loss: 17.1939\n",
      "Text 89/300\n",
      "Epoch 1/1 - Loss: 17.6954\n",
      "Text 90/300\n",
      "Epoch 1/1 - Loss: 17.2709\n",
      "Text 91/300\n",
      "Epoch 1/1 - Loss: 13.9249\n",
      "Text 92/300\n",
      "Epoch 1/1 - Loss: 16.1016\n",
      "Text 93/300\n",
      "Epoch 1/1 - Loss: 17.0809\n",
      "Text 94/300\n",
      "Epoch 1/1 - Loss: 18.6728\n",
      "Text 95/300\n",
      "Epoch 1/1 - Loss: 17.1586\n",
      "Text 96/300\n",
      "Epoch 1/1 - Loss: 15.2492\n",
      "Text 97/300\n",
      "Epoch 1/1 - Loss: 15.9157\n",
      "Text 98/300\n",
      "Epoch 1/1 - Loss: 16.4058\n",
      "Text 99/300\n",
      "Epoch 1/1 - Loss: 16.0024\n",
      "Text 100/300\n",
      "Epoch 1/1 - Loss: 16.0236\n",
      "Text 101/300\n",
      "Epoch 1/1 - Loss: 16.2727\n",
      "Text 102/300\n",
      "Epoch 1/1 - Loss: 16.3335\n",
      "Text 103/300\n",
      "Epoch 1/1 - Loss: 17.7888\n",
      "Text 104/300\n",
      "Epoch 1/1 - Loss: 15.6322\n",
      "Text 105/300\n",
      "Epoch 1/1 - Loss: 16.9378\n",
      "Text 106/300\n",
      "Epoch 1/1 - Loss: 16.4197\n",
      "Text 107/300\n",
      "Epoch 1/1 - Loss: 16.5374\n",
      "Text 108/300\n",
      "Epoch 1/1 - Loss: 15.4954\n",
      "Text 109/300\n",
      "Epoch 1/1 - Loss: 15.3008\n",
      "Text 110/300\n",
      "Epoch 1/1 - Loss: 18.5662\n",
      "Text 111/300\n",
      "Epoch 1/1 - Loss: 18.9194\n",
      "Text 112/300\n",
      "Epoch 1/1 - Loss: 16.4902\n",
      "Text 113/300\n",
      "Epoch 1/1 - Loss: 15.9184\n",
      "Text 114/300\n",
      "Epoch 1/1 - Loss: 15.9927\n",
      "Text 115/300\n",
      "Epoch 1/1 - Loss: 7.1924\n",
      "Text 116/300\n",
      "Epoch 1/1 - Loss: 14.3911\n",
      "Text 117/300\n",
      "Epoch 1/1 - Loss: 18.3668\n",
      "Text 118/300\n",
      "Epoch 1/1 - Loss: 15.9869\n",
      "Text 119/300\n",
      "Epoch 1/1 - Loss: 16.8575\n",
      "Text 120/300\n",
      "Epoch 1/1 - Loss: 17.5135\n",
      "Text 121/300\n",
      "Epoch 1/1 - Loss: 15.5761\n",
      "Text 122/300\n",
      "Epoch 1/1 - Loss: 17.5046\n",
      "Text 123/300\n",
      "Epoch 1/1 - Loss: 16.7773\n",
      "Text 124/300\n",
      "Epoch 1/1 - Loss: 15.6645\n",
      "Text 125/300\n",
      "Epoch 1/1 - Loss: 16.2241\n",
      "Text 126/300\n",
      "Epoch 1/1 - Loss: 16.6186\n",
      "Text 127/300\n",
      "Epoch 1/1 - Loss: 16.8451\n",
      "Text 128/300\n",
      "Epoch 1/1 - Loss: 15.4255\n",
      "Text 129/300\n",
      "Epoch 1/1 - Loss: 14.9059\n",
      "Text 130/300\n",
      "Epoch 1/1 - Loss: 15.9450\n",
      "Text 131/300\n",
      "Epoch 1/1 - Loss: 10.4795\n",
      "Text 132/300\n",
      "Epoch 1/1 - Loss: 15.7439\n",
      "Text 133/300\n",
      "Epoch 1/1 - Loss: 16.7250\n",
      "Text 134/300\n",
      "Epoch 1/1 - Loss: 16.4628\n",
      "Text 135/300\n",
      "Epoch 1/1 - Loss: 15.4670\n",
      "Text 136/300\n",
      "Epoch 1/1 - Loss: 18.6211\n",
      "Text 137/300\n",
      "Epoch 1/1 - Loss: 16.2888\n",
      "Text 138/300\n",
      "Epoch 1/1 - Loss: 15.0821\n",
      "Text 139/300\n",
      "Epoch 1/1 - Loss: 15.4429\n",
      "Text 140/300\n",
      "Epoch 1/1 - Loss: 14.8781\n",
      "Text 141/300\n",
      "Epoch 1/1 - Loss: 15.6639\n",
      "Text 142/300\n",
      "Epoch 1/1 - Loss: 16.2047\n",
      "Text 143/300\n",
      "Epoch 1/1 - Loss: 17.1264\n",
      "Text 144/300\n",
      "Epoch 1/1 - Loss: 15.3025\n",
      "Text 145/300\n",
      "Epoch 1/1 - Loss: 17.0881\n",
      "Text 146/300\n",
      "Epoch 1/1 - Loss: 16.6633\n",
      "Text 147/300\n",
      "Epoch 1/1 - Loss: 17.1070\n",
      "Text 148/300\n",
      "Epoch 1/1 - Loss: 10.7225\n",
      "Text 149/300\n",
      "Epoch 1/1 - Loss: 14.7693\n",
      "Text 150/300\n",
      "Epoch 1/1 - Loss: 16.1280\n",
      "Text 151/300\n",
      "Epoch 1/1 - Loss: 17.0162\n",
      "Text 152/300\n",
      "Epoch 1/1 - Loss: 14.5462\n",
      "Text 153/300\n",
      "Epoch 1/1 - Loss: 17.8081\n",
      "Text 154/300\n",
      "Epoch 1/1 - Loss: 14.7258\n",
      "Text 155/300\n",
      "Epoch 1/1 - Loss: 17.6924\n",
      "Text 156/300\n",
      "Epoch 1/1 - Loss: 15.1860\n",
      "Text 157/300\n",
      "Epoch 1/1 - Loss: 16.1453\n",
      "Text 158/300\n",
      "Epoch 1/1 - Loss: 16.2147\n",
      "Text 159/300\n",
      "Epoch 1/1 - Loss: 16.3384\n",
      "Text 160/300\n",
      "Epoch 1/1 - Loss: 15.6414\n",
      "Text 161/300\n",
      "Epoch 1/1 - Loss: 15.5323\n",
      "Text 162/300\n",
      "Epoch 1/1 - Loss: 16.4494\n",
      "Text 163/300\n",
      "Epoch 1/1 - Loss: 16.1396\n",
      "Text 164/300\n",
      "Epoch 1/1 - Loss: 16.4069\n",
      "Text 165/300\n",
      "Epoch 1/1 - Loss: 15.0383\n",
      "Text 166/300\n",
      "Epoch 1/1 - Loss: 15.3602\n",
      "Text 167/300\n",
      "Epoch 1/1 - Loss: 16.4037\n",
      "Text 168/300\n",
      "Epoch 1/1 - Loss: 16.5228\n",
      "Text 169/300\n",
      "Epoch 1/1 - Loss: 15.7546\n",
      "Text 170/300\n",
      "Epoch 1/1 - Loss: 8.1039\n",
      "Text 171/300\n",
      "Epoch 1/1 - Loss: 15.3349\n",
      "Text 172/300\n",
      "Epoch 1/1 - Loss: 15.5294\n",
      "Text 173/300\n",
      "Epoch 1/1 - Loss: 16.2644\n",
      "Text 174/300\n",
      "Epoch 1/1 - Loss: 17.2317\n",
      "Text 175/300\n",
      "Epoch 1/1 - Loss: 10.9638\n",
      "Text 176/300\n",
      "Epoch 1/1 - Loss: 16.7945\n",
      "Text 177/300\n",
      "Epoch 1/1 - Loss: 17.8191\n",
      "Text 178/300\n",
      "Epoch 1/1 - Loss: 14.1477\n",
      "Text 179/300\n",
      "Epoch 1/1 - Loss: 16.5592\n",
      "Text 180/300\n",
      "Epoch 1/1 - Loss: 13.8074\n",
      "Text 181/300\n",
      "Epoch 1/1 - Loss: 14.5548\n",
      "Text 182/300\n",
      "Epoch 1/1 - Loss: 14.9980\n",
      "Text 183/300\n",
      "Epoch 1/1 - Loss: 16.3700\n",
      "Text 184/300\n",
      "Epoch 1/1 - Loss: 15.6083\n",
      "Text 185/300\n",
      "Epoch 1/1 - Loss: 14.8735\n",
      "Text 186/300\n",
      "Epoch 1/1 - Loss: 16.2743\n",
      "Text 187/300\n",
      "Epoch 1/1 - Loss: 16.0936\n",
      "Text 188/300\n",
      "Epoch 1/1 - Loss: 15.6848\n",
      "Text 189/300\n",
      "Epoch 1/1 - Loss: 15.7179\n",
      "Text 190/300\n",
      "Epoch 1/1 - Loss: 14.6463\n",
      "Text 191/300\n",
      "Epoch 1/1 - Loss: 16.1579\n",
      "Text 192/300\n",
      "Epoch 1/1 - Loss: 16.3054\n",
      "Text 193/300\n",
      "Epoch 1/1 - Loss: 12.6596\n",
      "Text 194/300\n",
      "Epoch 1/1 - Loss: 15.9779\n",
      "Text 195/300\n",
      "Epoch 1/1 - Loss: 17.1700\n",
      "Text 196/300\n",
      "Epoch 1/1 - Loss: 16.5849\n",
      "Text 197/300\n",
      "Epoch 1/1 - Loss: 15.9652\n",
      "Text 198/300\n",
      "Epoch 1/1 - Loss: 17.6060\n",
      "Text 199/300\n",
      "Epoch 1/1 - Loss: 14.9514\n",
      "Text 200/300\n",
      "Epoch 1/1 - Loss: 15.6535\n",
      "Text 201/300\n",
      "Epoch 1/1 - Loss: 15.2045\n",
      "Text 202/300\n",
      "Epoch 1/1 - Loss: 16.2429\n",
      "Text 203/300\n",
      "Epoch 1/1 - Loss: 17.5552\n",
      "Text 204/300\n",
      "Epoch 1/1 - Loss: 15.6379\n",
      "Text 205/300\n",
      "Epoch 1/1 - Loss: 16.5977\n",
      "Text 206/300\n",
      "Epoch 1/1 - Loss: 14.2949\n",
      "Text 207/300\n",
      "Epoch 1/1 - Loss: 14.2248\n",
      "Text 208/300\n",
      "Epoch 1/1 - Loss: 14.9002\n",
      "Text 209/300\n",
      "Epoch 1/1 - Loss: 17.1782\n",
      "Text 210/300\n",
      "Epoch 1/1 - Loss: 17.3415\n",
      "Text 211/300\n",
      "Epoch 1/1 - Loss: 16.1579\n",
      "Text 212/300\n",
      "Epoch 1/1 - Loss: 17.3529\n",
      "Text 213/300\n",
      "Epoch 1/1 - Loss: 15.8850\n",
      "Text 214/300\n",
      "Epoch 1/1 - Loss: 16.0916\n",
      "Text 215/300\n",
      "Epoch 1/1 - Loss: 14.1323\n",
      "Text 216/300\n",
      "Epoch 1/1 - Loss: 14.5414\n",
      "Text 217/300\n",
      "Epoch 1/1 - Loss: 15.8716\n",
      "Text 218/300\n",
      "Epoch 1/1 - Loss: 15.4341\n",
      "Text 219/300\n",
      "Epoch 1/1 - Loss: 14.4023\n",
      "Text 220/300\n",
      "Epoch 1/1 - Loss: 16.2064\n",
      "Text 221/300\n",
      "Epoch 1/1 - Loss: 17.7017\n",
      "Text 222/300\n",
      "Epoch 1/1 - Loss: 16.0857\n",
      "Text 223/300\n",
      "Epoch 1/1 - Loss: 15.2430\n",
      "Text 224/300\n",
      "Epoch 1/1 - Loss: 15.9038\n",
      "Text 225/300\n",
      "Epoch 1/1 - Loss: 18.2279\n",
      "Text 226/300\n",
      "Epoch 1/1 - Loss: 15.7645\n",
      "Text 227/300\n",
      "Epoch 1/1 - Loss: 17.1865\n",
      "Text 228/300\n",
      "Epoch 1/1 - Loss: 15.7086\n",
      "Text 229/300\n",
      "Epoch 1/1 - Loss: 15.0604\n",
      "Text 230/300\n",
      "Epoch 1/1 - Loss: 15.9184\n",
      "Text 231/300\n",
      "Epoch 1/1 - Loss: 15.2314\n",
      "Text 232/300\n",
      "Epoch 1/1 - Loss: 15.8076\n",
      "Text 233/300\n",
      "Epoch 1/1 - Loss: 16.2080\n",
      "Text 234/300\n",
      "Epoch 1/1 - Loss: 17.4551\n",
      "Text 235/300\n",
      "Epoch 1/1 - Loss: 15.9573\n",
      "Text 236/300\n",
      "Epoch 1/1 - Loss: 15.8970\n",
      "Text 237/300\n",
      "Epoch 1/1 - Loss: 15.2470\n",
      "Text 238/300\n",
      "Epoch 1/1 - Loss: 16.1540\n",
      "Text 239/300\n",
      "Epoch 1/1 - Loss: 15.1473\n",
      "Text 240/300\n",
      "Epoch 1/1 - Loss: 15.5227\n",
      "Text 241/300\n",
      "Epoch 1/1 - Loss: 13.0055\n",
      "Text 242/300\n",
      "Epoch 1/1 - Loss: 15.2583\n",
      "Text 243/300\n",
      "Epoch 1/1 - Loss: 15.8335\n",
      "Text 244/300\n",
      "Epoch 1/1 - Loss: 17.2901\n",
      "Text 245/300\n",
      "Epoch 1/1 - Loss: 16.6142\n",
      "Text 246/300\n",
      "Epoch 1/1 - Loss: 15.1509\n",
      "Text 247/300\n",
      "Epoch 1/1 - Loss: 15.8431\n",
      "Text 248/300\n",
      "Epoch 1/1 - Loss: 15.9099\n",
      "Text 249/300\n",
      "Epoch 1/1 - Loss: 17.9305\n",
      "Text 250/300\n",
      "Epoch 1/1 - Loss: 16.0431\n",
      "Text 251/300\n",
      "Epoch 1/1 - Loss: 16.5315\n",
      "Text 252/300\n",
      "Epoch 1/1 - Loss: 15.7683\n",
      "Text 253/300\n",
      "Epoch 1/1 - Loss: 16.0918\n",
      "Text 254/300\n",
      "Epoch 1/1 - Loss: 16.6278\n",
      "Text 255/300\n",
      "Epoch 1/1 - Loss: 17.0960\n",
      "Text 256/300\n",
      "Epoch 1/1 - Loss: 15.4960\n",
      "Text 257/300\n",
      "Epoch 1/1 - Loss: 16.5468\n",
      "Text 258/300\n",
      "Epoch 1/1 - Loss: 17.2171\n",
      "Text 259/300\n",
      "Epoch 1/1 - Loss: 16.9670\n",
      "Text 260/300\n",
      "Epoch 1/1 - Loss: 15.5158\n",
      "Text 261/300\n",
      "Epoch 1/1 - Loss: 15.7609\n",
      "Text 262/300\n",
      "Epoch 1/1 - Loss: 17.6907\n",
      "Text 263/300\n",
      "Epoch 1/1 - Loss: 18.2669\n",
      "Text 264/300\n",
      "Epoch 1/1 - Loss: 13.8709\n",
      "Text 265/300\n",
      "Epoch 1/1 - Loss: 17.1482\n",
      "Text 266/300\n",
      "Epoch 1/1 - Loss: 16.5519\n",
      "Text 267/300\n",
      "Epoch 1/1 - Loss: 16.7044\n",
      "Text 268/300\n",
      "Epoch 1/1 - Loss: 18.7663\n",
      "Text 269/300\n",
      "Epoch 1/1 - Loss: 15.4831\n",
      "Text 270/300\n",
      "Epoch 1/1 - Loss: 16.9225\n",
      "Text 271/300\n",
      "Epoch 1/1 - Loss: 16.5792\n",
      "Text 272/300\n",
      "Epoch 1/1 - Loss: 16.3253\n",
      "Text 273/300\n",
      "Epoch 1/1 - Loss: 15.8546\n",
      "Text 274/300\n",
      "Epoch 1/1 - Loss: 16.0745\n",
      "Text 275/300\n",
      "Epoch 1/1 - Loss: 15.8587\n",
      "Text 276/300\n",
      "Epoch 1/1 - Loss: 17.1774\n",
      "Text 277/300\n",
      "Epoch 1/1 - Loss: 16.6002\n",
      "Text 278/300\n",
      "Epoch 1/1 - Loss: 17.2752\n",
      "Text 279/300\n",
      "Epoch 1/1 - Loss: 15.6373\n",
      "Text 280/300\n",
      "Epoch 1/1 - Loss: 14.8566\n",
      "Text 281/300\n",
      "Epoch 1/1 - Loss: 16.0865\n",
      "Text 282/300\n",
      "Epoch 1/1 - Loss: 16.2639\n",
      "Text 283/300\n",
      "Epoch 1/1 - Loss: 15.7575\n",
      "Text 284/300\n",
      "Epoch 1/1 - Loss: 15.7229\n",
      "Text 285/300\n",
      "Epoch 1/1 - Loss: 16.5428\n",
      "Text 286/300\n",
      "Epoch 1/1 - Loss: 15.9169\n",
      "Text 287/300\n",
      "Epoch 1/1 - Loss: 15.2310\n",
      "Text 288/300\n",
      "Epoch 1/1 - Loss: 16.9035\n",
      "Text 289/300\n",
      "Epoch 1/1 - Loss: 15.2656\n",
      "Text 290/300\n",
      "Epoch 1/1 - Loss: 16.0364\n",
      "Text 291/300\n",
      "Epoch 1/1 - Loss: 14.8537\n",
      "Text 292/300\n",
      "Epoch 1/1 - Loss: 16.0761\n",
      "Text 293/300\n",
      "Epoch 1/1 - Loss: 17.8639\n",
      "Text 294/300\n",
      "Epoch 1/1 - Loss: 16.2800\n",
      "Text 295/300\n",
      "Epoch 1/1 - Loss: 14.7215\n",
      "Text 296/300\n",
      "Epoch 1/1 - Loss: 17.5608\n",
      "Text 297/300\n",
      "Epoch 1/1 - Loss: 16.0623\n",
      "Text 298/300\n",
      "Epoch 1/1 - Loss: 17.3112\n",
      "Text 299/300\n",
      "Epoch 1/1 - Loss: 15.8951\n",
      "Text 300/300\n",
      "Epoch 1/1 - Loss: 16.5332\n",
      "[['coronavirus', 1.0], ['acids', 0.9846713542938232], ['pyramidal', 0.9814345836639404], ['levels', 0.9786362051963806], ['dominant', 0.9772925972938538], ['clinical', 0.9732319116592407], ['risk', 0.9709049463272095], ['induction', 0.9703773856163025], ['response', 0.9672532677650452], ['protein', 0.9611968398094177], ['sex', 0.9592359662055969], ['stress', 0.956969141960144], ['pathway', 0.956817090511322], ['series', 0.9561311602592468], ['controlling', 0.955571174621582], ['mediated', 0.9544868469238281], ['pair', 0.9542061686515808], ['responses', 0.9528378248214722], ['isoform', 0.9490906596183777], ['consumption', 0.948040246963501], ['episodes', 0.944985568523407], ['congenital', 0.9438966512680054], ['border', 0.942502498626709], ['cavity', 0.940731406211853], ['sequencing', 0.9403350353240967], ['side', 0.9388408660888672], ['location', 0.9370112419128418], ['deficiency', 0.9365637302398682], ['Blood', 0.9337650537490845], ['on', 0.9326011538505554], ['communicating', 0.9317930936813354], ['H', 0.9309316873550415], ['finding', 0.9295977354049683], ['epithelium', 0.9295673966407776], ['horizontal', 0.927777886390686], ['exchange', 0.9276842474937439], ['necrotic', 0.927448034286499], ['temporal', 0.9234602451324463], ['female', 0.9206811189651489], ['great', 0.920234739780426], ['formation', 0.9182448983192444], ['food', 0.9181005358695984], ['quantity', 0.9176468849182129], ['Food', 0.9166758060455322], ['group', 0.9139241576194763], ['square', 0.9139063358306885], ['degenerative', 0.9094197154045105], ['soon', 0.9093082547187805], ['activation', 0.9092742800712585], ['top', 0.9081902503967285], ['center', 0.9079980254173279], ['yellow', 0.9075960516929626], ['plate', 0.9063540697097778], ['partial', 0.9062147736549377], ['epithelial', 0.9054071307182312], ['dental', 0.9032787084579468], ['false', 0.9017605185508728], ['placed', 0.9017069339752197], ['inflammation', 0.901320219039917], ['CNS', 0.9007351994514465], ['killing', 0.9005593061447144], ['impacted', 0.9004366993904114], ['compensatory', 0.8998633027076721], ['cytokine', 0.8988344669342041], ['new', 0.8986932039260864], ['O', 0.8981605172157288], ['cardiovascular', 0.8952153325080872], ['leukemia', 0.8927304148674011], ['density', 0.8926144242286682], ['selective', 0.8914521932601929], ['transporter', 0.8906699419021606], ['intracellular', 0.8893540501594543], ['during', 0.8892326354980469], ['formed', 0.8887937664985657], ['contralateral', 0.8848212361335754], ['from', 0.884656548500061], ['minutes', 0.8841261863708496], ['ability', 0.8831536769866943], ['postsynaptic', 0.8827271461486816], ['fragmentation', 0.882317304611206], ['upon', 0.8817331194877625], ['mammalian', 0.8806968927383423], ['moderate', 0.8786216974258423], ['molecular', 0.8774070739746094], ['antimicrobial', 0.8751852512359619], ['human', 0.8725776076316833], ['application', 0.8685633540153503], ['glial', 0.867652416229248], ['immunodeficiency', 0.8662642240524292], ['except', 0.8643298745155334], ['particle', 0.8632931709289551], ['Two', 0.8621155023574829], ['separation', 0.8608949780464172], ['triangle', 0.8605768084526062], ['fatty', 0.8589354753494263], ['amplification', 0.8583675026893616], ['modification', 0.8576393127441406], ['stratified', 0.854507327079773], ['basal', 0.8537065386772156], ['organization', 0.8535141348838806]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Biomedical Entities 文件夹目录\n",
    "OwlDirPath = \"/home/tanglongbin/NLP/owl\"\n",
    "# owl文件数量\n",
    "OwlFileNum = 3\n",
    "# 分词文件夹目录\n",
    "TokensDirPath = \"/home/tanglongbin/NLP/nltk_text\"\n",
    "# 作为词汇表的Tokens文件数量\n",
    "TokensFileNum = 50\n",
    "# 词汇量\n",
    "VocabSize = 10000\n",
    "\n",
    "# 文章目录（split_text目录）\n",
    "TextDirPath = \"/home/tanglongbin/NLP/split_text\"\n",
    "# 用于训练的文章数量\n",
    "TextNum = 300\n",
    "# 每篇文章训练的次数\n",
    "EpochsNum = 1\n",
    "# 中心词一侧Positive单词的数量\n",
    "WindowSize = 5\n",
    "# Negative单词的数量\n",
    "NegaNum = 2\n",
    "# 词向量Feature维度\n",
    "EmbedSize = 300\n",
    "\n",
    "# 单次训练使用的数量（可更具内存/显卡负载进行调整）\n",
    "Batch_Size = 512\n",
    "# 学习速率（步长）\n",
    "LearningRate = 0.001\n",
    "\n",
    "\n",
    "\n",
    "# 获取指定文件夹内的所有文件的绝对路径\n",
    "# return type: List\n",
    "def GetFilePath(DirPath):\n",
    "    \n",
    "    Res = []\n",
    "    for FilePath, DirNames, FileNames in os.walk(DirPath):\n",
    "        for FileName in FileNames:\n",
    "            str_tmp = os.path.join(FilePath, FileName)\n",
    "            Res.append(str_tmp)\n",
    "            \n",
    "    return Res\n",
    "\n",
    "\n",
    "# 获取Vocabulary\n",
    "# 返回设定词汇量的vocab字典\n",
    "def GetVocab():\n",
    "    TmpList = []\n",
    "    owl_list = []\n",
    "    TokenList = []\n",
    "    OwlFiles = GetFilePath(OwlDirPath)\n",
    "    for i in range(OwlFileNum):\n",
    "        with open(OwlFiles[i], 'r') as f:\n",
    "            S = f.readline()\n",
    "            # String to List\n",
    "            L = eval(S)\n",
    "            TmpList += L\n",
    "    TmpList = [x[0] for x in TmpList]\n",
    "    for item in TmpList:\n",
    "        owl_list += item.split()\n",
    "    \n",
    "    TmpList = []\n",
    "    TokensFiles = GetFilePath(TokensDirPath)\n",
    "    for i in range(TokensFileNum):\n",
    "        with open(TokensFiles[i], 'r') as f:\n",
    "            S = f.readline()\n",
    "            # String to List\n",
    "            L = eval(S)\n",
    "            TmpList += L\n",
    "    \n",
    "    for item in TmpList:\n",
    "        if item in owl_list:\n",
    "            TokenList.append(item) \n",
    "    \n",
    "    # 获取频率最高的VocabSize-1个单词，并将剩下的单词归类为'<unk>'\n",
    "    Vocab = dict(Counter(TokenList).most_common(VocabSize-1))\n",
    "    Vocab[\"<unk>\"] = len(TokenList) - np.sum(list(Vocab.values()))\n",
    "    return Vocab\n",
    "\n",
    "\n",
    "# 对Vocab里的单词进行编码和词频处理\n",
    "def AnalyzeVocab(Vocab):\n",
    "    # encoder & decoder\n",
    "    idx_to_word = [word for word in Vocab.keys()] \n",
    "    word_to_idx = {word:i for i, word in enumerate(idx_to_word)}\n",
    "    \n",
    "    # 词频变换 \n",
    "    word_counts = np.array([count for count in Vocab.values()], dtype=np.float32)\n",
    "    word_freqs = word_counts / np.sum(word_counts)\n",
    "    word_freqs = word_freqs ** (3./4.)\n",
    "    word_freqs = word_freqs / np.sum(word_freqs)  # 用来做 negative sampling\n",
    "    \n",
    "    return word_to_idx, idx_to_word, word_freqs\n",
    "\n",
    "\n",
    "# 创建Dataset, 并对文章进行编码\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, text, word_to_idx, idx_to_word, word_freqs):\n",
    "        super(Dataset, self).__init__()\n",
    "        # 编码文章\n",
    "        self.text_encoded = [word_to_idx.get(word, word_to_idx[\"<unk>\"]) for word in text]\n",
    "        self.word_to_idx = word_to_idx \n",
    "        self.idx_to_word = idx_to_word\n",
    "        # 转换为Tensor以便于GPU训练\n",
    "        self.text_encoded = torch.Tensor(self.text_encoded).long()\n",
    "        self.word_freqs = torch.Tensor(word_freqs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_encoded)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ''' 这个function返回以下数据用于训练\n",
    "            - 中心词\n",
    "            - 这个单词附近的(positive)单词\n",
    "            - 随机采样的K个单词作为negative sample\n",
    "        '''\n",
    "        center_word = self.text_encoded[idx] \n",
    "        pos_indices = list(range(idx-WindowSize, idx)) + list(range(idx + 1, idx + WindowSize + 1))\n",
    "        pos_indices = [i % len(self.text_encoded) for i in pos_indices]\n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "        # replacement=False无放回的取\n",
    "        neg_words = torch.multinomial(self.word_freqs, (NegaNum + 1) * pos_words.shape[0], replacement=False)\n",
    "        # Neg_words 与 pos_words 取差集，确保 neg_words 中不包含 pos_words\n",
    "        neg_words = np.setdiff1d(neg_words.numpy(), pos_words.numpy())\n",
    "        neg_words = neg_words[:NegaNum * pos_words.shape[0]]\n",
    "        neg_words = torch.Tensor(neg_words)\n",
    "    \n",
    "        return center_word, pos_words, neg_words \n",
    "\n",
    "\n",
    "# Skip-Gram Model\n",
    "class EmbeddingModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        # 模型建立 & 初始化\n",
    "        self.embed = torch.nn.Embedding(vocab_size, embed_size)\n",
    "        initrange = 0.5 / embed_size\n",
    "        self.embed.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        '''\n",
    "        input_labels: 中心词,         [batch_size]\n",
    "        pos_labels: 中心词周围词       [batch_size * (WindowSize * 2)]\n",
    "        neg_labelss: 中心词负采样单词  [batch_size, (WindowSize * 2 * NegaNum)]\n",
    "        return: loss, 返回loss        [batch_size]\n",
    "        '''\n",
    "        \n",
    "        # 编码\n",
    "        input_embedding = self.embed(input_labels)\n",
    "        input_embedding = input_embedding.unsqueeze(dim = 2)\n",
    "        pos_embedding = self.embed(pos_labels)\n",
    "        neg_embedding = self.embed(neg_labels)\n",
    "        \n",
    "\n",
    "        # torch.bmm()为batch间的矩阵相乘（b,n.m)*(b,m,p)=(b,n,p)\n",
    "        loss_pos = torch.bmm(pos_embedding, input_embedding).squeeze()\n",
    "        loss_neg = torch.bmm(neg_embedding, -input_embedding).squeeze()\n",
    "        \n",
    "        # loss计算\n",
    "        loss_pos = loss_pos.sigmoid().log().sum(dim = 1)\n",
    "        loss_neg = loss_neg.sigmoid().log().sum(dim = 1)\n",
    "        loss = (loss_pos + loss_neg).mean(dim = 0)\n",
    "        return -loss\n",
    "    \n",
    "    def Embeddings(self):\n",
    "        return self.embed.weight.data.cpu().numpy()\n",
    "\n",
    "\n",
    "# 返回与 Input_word 最相似的 Word_num 个单词\n",
    "# 返回一个二维 List[Word_num][1]\n",
    "# List[Word_num][0] 为单词，List[Word_num][1] 为相似度\n",
    "def CosineSimilarity(Model, word_to_idx, idx_to_word, Input_word, Word_num):\n",
    "    # 创建 Model Copy\n",
    "    W = Model.embed.weight.data.clone()\n",
    "    # 单位化\n",
    "    norm = W.norm(dim = 1).unsqueeze(dim = 1)\n",
    "    W = W/norm\n",
    "    \n",
    "    # 获取 Input_word 词向量\n",
    "    ids = [word_to_idx.get(Input_word, word_to_idx[\"<unk>\"])]\n",
    "    x = W[ids]\n",
    "    # 计算 similarity\n",
    "    similarity = torch.mm(x, W.T)\n",
    "    \n",
    "    topk = (-similarity[0,:]).argsort()[:Word_num]\n",
    "    Res = [[idx_to_word[j.item()], similarity[0][j.item()].item()] for j in topk]\n",
    "    \n",
    "    return Res\n",
    "\n",
    "\n",
    "# 返回与 Input_word 距离最小的 Word_num 个单词\n",
    "# 返回一个二维 List[Word_num][1]\n",
    "# List[Word_num][0] 为单词，List[Word_num][1] 为距离\n",
    "def FindNearest(Model, word_to_idx, idx_to_word, Input_word, Word_num):\n",
    "    # 创建 Model Copy\n",
    "    W = Model.embed.weight.data.clone()\n",
    "    # 获取 Input_word 词向量\n",
    "    ids = [word_to_idx.get(Input_word, word_to_idx[\"<unk>\"])]\n",
    "    x = W[ids]\n",
    "    \n",
    "    # 计算所有30000个embedding向量与传入单词embedding向量的相似度距离\n",
    "    Distance = torch.nn.PairwiseDistance(p=2)\n",
    "    cos_dis = Distance(x,W)\n",
    "    topk = cos_dis.argsort()[:Word_num]\n",
    "    Res = [[idx_to_word[j.item()], cos_dis[j.item()].item()] for j in topk]\n",
    "    \n",
    "    return Res\n",
    "\n",
    "\n",
    "def  StartTraining():\n",
    "    \n",
    "    # 获取Vocab\n",
    "    Vocab = GetVocab()\n",
    "    # 获得Vocab分析数据\n",
    "    word_to_idx, idx_to_word, word_freqs = AnalyzeVocab(Vocab)\n",
    "    # print(word_to_idx, idx_to_word, word_freqs)\n",
    "    # return\n",
    "    # 创建模型(使用Adam算法)\n",
    "    Model = EmbeddingModel(VocabSize, EmbedSize)\n",
    "    if torch.cuda.is_available():\n",
    "        # 若支持cuda加速则使用GPU训练\n",
    "        Model = Model.cuda()\n",
    "    Optimizer = torch.optim.Adam(Model.parameters(), lr = LearningRate)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 读取Text\n",
    "    TextFiles = GetFilePath(TextDirPath)\n",
    "    for i in range(TextNum):\n",
    "        with open(TextFiles[i]) as f:\n",
    "            S = f.readline()\n",
    "            # String to List\n",
    "            Text = eval(S)\n",
    "            \n",
    "            # 创建DataLoader\n",
    "            dataset = Dataset(Text, word_to_idx, idx_to_word, word_freqs)\n",
    "            dataloader = torch.utils.data.DataLoader(dataset, batch_size = Batch_Size, shuffle = True, num_workers = 2)\n",
    "            \n",
    "            print(f'Text {i+1}/{TextNum}')\n",
    "            # StartTraining\n",
    "            for j in range(EpochsNum):\n",
    "                for k, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "                    # 转换为 long int 类型\n",
    "                    input_labels = input_labels.long()\n",
    "                    pos_labels = pos_labels.long()\n",
    "                    neg_labels = neg_labels.long()\n",
    "                    if torch.cuda.is_available(): \n",
    "                        # 若支持cuda加速则使用GPU训练\n",
    "                        input_labels = input_labels.cuda()\n",
    "                        pos_labels = pos_labels.cuda()\n",
    "                        neg_labels = neg_labels.cuda()\n",
    "\n",
    "                    loss = Model(input_labels, pos_labels, neg_labels)\n",
    "                    Optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    Optimizer.step()\n",
    "        \n",
    "                print(f'Epoch {j+1}/{EpochsNum} - Loss: {loss.item():.4f}')\n",
    "    print(CosineSimilarity(Model, word_to_idx, idx_to_word, \"coronavirus\", 100))\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # 开始训练\n",
    "    StartTraining()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
